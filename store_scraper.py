#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åº—èˆ—æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v1.0
=====================================
ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰åº—èˆ—æƒ…å ±ï¼ˆåº—èˆ—åãƒ»ä½æ‰€ãƒ»é›»è©±ç•ªå·ãƒ»URLï¼‰ã‚’è‡ªå‹•æŠ½å‡º

ã€ä½¿ã„æ–¹ã€‘
1. ç’°å¢ƒæº–å‚™:
   pip install -r requirements.txt
   playwright install chromium

2. ä¼æ¥­URLä¸€è¦§ã‚’æº–å‚™:
   - company_urls.txt ã«1è¡Œ1URLã§è¨˜è¼‰
   - ã¾ãŸã¯ company_urls.csv ã‚’ä½œæˆï¼ˆä¼æ¥­å,URL ã®å½¢å¼ï¼‰

3. å®Ÿè¡Œ:
   python store_scraper.py

4. çµæœ:
   - output/åº—èˆ—ä¸€è¦§_YYYYMMDD_HHMMSS.csv
   - output/åº—èˆ—ä¸€è¦§_YYYYMMDD_HHMMSS.xlsx

ã€å¿…è¦ãªAPIã‚­ãƒ¼ã€‘
- GOOGLE_API_KEY: Gemini APIç”¨ï¼ˆ~/.env.local ã«è¨­å®šï¼‰

ã€æ³¨æ„äº‹é …ã€‘
- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’éµå®ˆã—ã¦ãã ã•ã„
- robots.txt ã‚’ç¢ºèªã—ã€è¨±å¯ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã®ã¿å–å¾—ã—ã¦ãã ã•ã„
- éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’é¿ã‘ã‚‹ãŸã‚ã€é©åˆ‡ãªé–“éš”ã‚’ç©ºã‘ã¦ãã ã•ã„
"""

import asyncio
import csv
import json
import os
import re
import sys
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Optional
from urllib.parse import urljoin, urlparse

import google.generativeai as genai
from dotenv import load_dotenv
from playwright.async_api import async_playwright, Page, Browser
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table

# ====================================
# è¨­å®š
# ====================================
console = Console()

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv(Path.home() / ".env.local")

# Gemini APIè¨­å®š
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    console.print("[bold red]ã‚¨ãƒ©ãƒ¼: GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“[/]")
    console.print("~/.env.local ã« GOOGLE_API_KEY=your_api_key ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
    sys.exit(1)

genai.configure(api_key=GOOGLE_API_KEY)

# Geminiãƒ¢ãƒ‡ãƒ«è¨­å®š
MODEL_NAME = "gemini-2.0-flash"  # é«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆ
model = genai.GenerativeModel(MODEL_NAME)

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
REQUEST_DELAY = 2.0  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
MAX_STORES_PER_COMPANY = 500  # 1ä¼æ¥­ã‚ãŸã‚Šã®æœ€å¤§åº—èˆ—æ•°
TIMEOUT = 30000  # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒŸãƒªç§’ï¼‰


# ====================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ====================================
@dataclass
class StoreInfo:
    """åº—èˆ—æƒ…å ±"""
    company_name: str  # ä¼æ¥­å
    store_name: str  # åº—èˆ—å
    address: str  # ä½æ‰€
    phone: str  # é›»è©±ç•ªå·
    url: str  # åº—èˆ—ãƒšãƒ¼ã‚¸URL
    prefecture: str = ""  # éƒ½é“åºœçœŒ
    business_hours: str = ""  # å–¶æ¥­æ™‚é–“ï¼ˆãŠã¾ã‘ï¼‰
    note: str = ""  # å‚™è€ƒ


@dataclass
class CompanyConfig:
    """ä¼æ¥­è¨­å®š"""
    name: str
    url: str
    store_list_pattern: str = ""  # åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰


# ====================================
# LLMè§£æ
# ====================================
async def find_store_list_page(page: Page, company_name: str) -> list[str]:
    """
    ãƒšãƒ¼ã‚¸å†…ã®ãƒªãƒ³ã‚¯ã‚’è§£æã—ã€åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã®å€™è£œURLã‚’è¿”ã™
    """
    # ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ä¸€è¦§ã‚’å–å¾—
    links = await page.evaluate("""
        () => {
            const anchors = document.querySelectorAll('a[href]');
            return Array.from(anchors).map(a => ({
                href: a.href,
                text: a.textContent.trim().substring(0, 100)
            })).filter(l => l.href && l.text);
        }
    """)

    if not links:
        return []

    # ãƒªãƒ³ã‚¯ä¸€è¦§ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ï¼ˆæœ€å¤§100ä»¶ï¼‰
    links_text = "\n".join([
        f"- {l['text']}: {l['href']}"
        for l in links[:100]
    ])

    prompt = f"""
ä»¥ä¸‹ã¯ä¼æ¥­ã€Œ{company_name}ã€ã®Webãƒšãƒ¼ã‚¸ã«ã‚ã‚‹ãƒªãƒ³ã‚¯ä¸€è¦§ã§ã™ã€‚
åº—èˆ—ä¸€è¦§ãƒ»åº—èˆ—æ¤œç´¢ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚

ã€ãƒªãƒ³ã‚¯ä¸€è¦§ã€‘
{links_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URLã®ã¿ã‚’JSONé…åˆ—ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å€™è£œãŒè¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã‚’æœ€å¤§3ã¤é¸ã‚“ã§ãã ã•ã„ã€‚
è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºã®é…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

ä¾‹: ["https://example.com/shops", "https://example.com/store-list"]
"""

    try:
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            generation_config={"temperature": 0.1}
        )

        # JSONã‚’æŠ½å‡º
        text = response.text.strip()
        # ```json ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†
        if "```" in text:
            match = re.search(r'\[.*?\]', text, re.DOTALL)
            if match:
                text = match.group()

        urls = json.loads(text)
        return urls if isinstance(urls, list) else []

    except Exception as e:
        console.print(f"[yellow]è­¦å‘Š: åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸æ¢ç´¢ã‚¨ãƒ©ãƒ¼: {e}[/]")
        return []


async def extract_store_info_from_html(
    html: str,
    company_name: str,
    base_url: str
) -> list[StoreInfo]:
    """
    HTMLã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’æŠ½å‡º
    """
    # HTMLãŒå¤§ãã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
    max_chars = 50000
    if len(html) > max_chars:
        html = html[:max_chars] + "\n... (ä»¥ä¸‹çœç•¥)"

    prompt = f"""
ä»¥ä¸‹ã®HTMLã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€ä¼æ¥­åã€‘{company_name}
ã€ãƒšãƒ¼ã‚¸URLã€‘{base_url}

ã€æŠ½å‡ºé …ç›®ã€‘
- store_name: åº—èˆ—å
- address: ä½æ‰€ï¼ˆéƒ½é“åºœçœŒã‹ã‚‰ç•ªåœ°ã¾ã§ï¼‰
- phone: é›»è©±ç•ªå·ï¼ˆãƒã‚¤ãƒ•ãƒ³ä»˜ãå½¢å¼ï¼‰
- prefecture: éƒ½é“åºœçœŒ
- business_hours: å–¶æ¥­æ™‚é–“ï¼ˆã‚ã‚Œã°ï¼‰
- url: åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸URLï¼ˆã‚ã‚Œã°ã€ãªã‘ã‚Œã°ç©ºæ–‡å­—ï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
JSONé…åˆ—ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚åº—èˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºé…åˆ— [] ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚

ä¾‹:
[
  {{
    "store_name": "æ¸‹è°·åº—",
    "address": "æ±äº¬éƒ½æ¸‹è°·åŒºé“ç„å‚1-2-3",
    "phone": "03-1234-5678",
    "prefecture": "æ±äº¬éƒ½",
    "business_hours": "10:00-21:00",
    "url": "https://example.com/shops/shibuya"
  }}
]

ã€HTMLã€‘
{html}
"""

    try:
        response = await asyncio.to_thread(
            model.generate_content,
            prompt,
            generation_config={"temperature": 0.1}
        )

        text = response.text.strip()

        # ```json ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†
        if "```" in text:
            match = re.search(r'\[.*\]', text, re.DOTALL)
            if match:
                text = match.group()

        stores_data = json.loads(text)

        if not isinstance(stores_data, list):
            return []

        stores = []
        for data in stores_data[:MAX_STORES_PER_COMPANY]:
            store = StoreInfo(
                company_name=company_name,
                store_name=data.get("store_name", ""),
                address=data.get("address", ""),
                phone=data.get("phone", ""),
                prefecture=data.get("prefecture", ""),
                business_hours=data.get("business_hours", ""),
                url=data.get("url", ""),
            )
            if store.store_name or store.address:  # æœ€ä½é™ã®æƒ…å ±ãŒã‚ã‚Œã°è¿½åŠ 
                stores.append(store)

        return stores

    except json.JSONDecodeError as e:
        console.print(f"[yellow]è­¦å‘Š: JSONè§£æã‚¨ãƒ©ãƒ¼: {e}[/]")
        return []
    except Exception as e:
        console.print(f"[yellow]è­¦å‘Š: åº—èˆ—æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}[/]")
        return []


# ====================================
# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
# ====================================
async def scrape_company(
    browser: Browser,
    company: CompanyConfig,
    progress: Progress,
    task_id
) -> list[StoreInfo]:
    """
    1ä¼æ¥­ã®åº—èˆ—æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    stores: list[StoreInfo] = []

    try:
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        page = await context.new_page()

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
        progress.update(task_id, description=f"[cyan]{company.name}[/] - ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ä¸­...")
        await page.goto(company.url, timeout=TIMEOUT, wait_until="networkidle")
        await asyncio.sleep(REQUEST_DELAY)

        # åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        progress.update(task_id, description=f"[cyan]{company.name}[/] - åº—èˆ—ä¸€è¦§ã‚’æ¢ç´¢ä¸­...")

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if company.store_list_pattern:
            store_list_urls = [urljoin(company.url, company.store_list_pattern)]
        else:
            store_list_urls = await find_store_list_page(page, company.name)

        if not store_list_urls:
            # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸è‡ªä½“ãŒåº—èˆ—ä¸€è¦§ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹
            console.print(f"[yellow]{company.name}: åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’è§£æã—ã¾ã™ã€‚[/]")
            store_list_urls = [company.url]

        # åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
        for store_url in store_list_urls[:3]:  # æœ€å¤§3ãƒšãƒ¼ã‚¸
            try:
                progress.update(task_id, description=f"[cyan]{company.name}[/] - {urlparse(store_url).path}")

                if store_url != company.url:
                    await page.goto(store_url, timeout=TIMEOUT, wait_until="networkidle")
                    await asyncio.sleep(REQUEST_DELAY)

                # ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
                html = await page.content()

                # åº—èˆ—æƒ…å ±ã‚’æŠ½å‡º
                progress.update(task_id, description=f"[cyan]{company.name}[/] - åº—èˆ—æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
                extracted = await extract_store_info_from_html(html, company.name, store_url)

                if extracted:
                    console.print(f"[green]{company.name}: {len(extracted)}ä»¶ã®åº—èˆ—ã‚’æŠ½å‡º[/]")
                    stores.extend(extracted)
                    break  # æˆåŠŸã—ãŸã‚‰çµ‚äº†

            except Exception as e:
                console.print(f"[yellow]{company.name}: ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({store_url}): {e}[/]")
                continue

        await context.close()

    except Exception as e:
        console.print(f"[red]{company.name}: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}[/]")

    return stores


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    console.print("\n[bold blue]ğŸª åº—èˆ—æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« v1.0[/]\n")

    # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    work_dir = Path(__file__).parent
    output_dir = work_dir / "output"
    output_dir.mkdir(exist_ok=True)

    # ä¼æ¥­URLä¸€è¦§ã‚’èª­ã¿è¾¼ã¿
    companies: list[CompanyConfig] = []

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
    csv_file = work_dir / "company_urls.csv"
    txt_file = work_dir / "company_urls.txt"

    if csv_file.exists():
        console.print(f"[dim]å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {csv_file}[/]")
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                companies.append(CompanyConfig(
                    name=row.get("ä¼æ¥­å", row.get("name", "")),
                    url=row.get("URL", row.get("url", "")),
                    store_list_pattern=row.get("åº—èˆ—ä¸€è¦§ãƒ‘ã‚¹", row.get("pattern", ""))
                ))
    elif txt_file.exists():
        console.print(f"[dim]å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {txt_file}[/]")
        with open(txt_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    # URLã‹ã‚‰ä¼æ¥­åã‚’æ¨æ¸¬
                    parsed = urlparse(line)
                    name = parsed.netloc.replace("www.", "")
                    companies.append(CompanyConfig(name=name, url=line))
    else:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        console.print("[yellow]ä¼æ¥­URLä¸€è¦§ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚[/]")

        sample_csv = """ä¼æ¥­å,URL,åº—èˆ—ä¸€è¦§ãƒ‘ã‚¹
ã‚¹ã‚¿ãƒ¼ãƒãƒƒã‚¯ã‚¹,https://www.starbucks.co.jp,/store/search/
ãƒã‚¯ãƒ‰ãƒŠãƒ«ãƒ‰,https://www.mcdonalds.co.jp,/shop/search/
ã‚»ãƒ–ãƒ³ã‚¤ãƒ¬ãƒ–ãƒ³,https://www.sej.co.jp,/shop/
"""
        with open(csv_file, "w", encoding="utf-8") as f:
            f.write(sample_csv)

        console.print(f"[green]ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {csv_file}[/]")
        console.print("ä¼æ¥­URLã‚’ç·¨é›†ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    if not companies:
        console.print("[red]å‡¦ç†å¯¾è±¡ã®ä¼æ¥­ãŒã‚ã‚Šã¾ã›ã‚“ã€‚[/]")
        return

    console.print(f"[bold]å‡¦ç†å¯¾è±¡: {len(companies)}ç¤¾[/]\n")

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    all_stores: list[StoreInfo] = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("{task.completed}/{task.total}"),
            console=console,
        ) as progress:

            task = progress.add_task("å‡¦ç†ä¸­...", total=len(companies))

            for company in companies:
                stores = await scrape_company(browser, company, progress, task)
                all_stores.extend(stores)
                progress.advance(task)

        await browser.close()

    # çµæœå‡ºåŠ›
    if not all_stores:
        console.print("\n[yellow]åº—èˆ—æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚[/]")
        return

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # CSVå‡ºåŠ›
    csv_output = output_dir / f"åº—èˆ—ä¸€è¦§_{timestamp}.csv"
    with open(csv_output, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=[
            "company_name", "store_name", "address", "prefecture",
            "phone", "business_hours", "url", "note"
        ])
        writer.writeheader()
        for store in all_stores:
            writer.writerow(asdict(store))

    console.print(f"\n[green]âœ… CSVå‡ºåŠ›: {csv_output}[/]")

    # Excelå‡ºåŠ›ï¼ˆopenpyxlãŒã‚ã‚Œã°ï¼‰
    try:
        import openpyxl
        from openpyxl.utils.dataframe import dataframe_to_rows

        wb = openpyxl.Workbook()
        ws = wb.active
        ws.title = "åº—èˆ—ä¸€è¦§"

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = ["ä¼æ¥­å", "åº—èˆ—å", "ä½æ‰€", "éƒ½é“åºœçœŒ", "é›»è©±ç•ªå·", "å–¶æ¥­æ™‚é–“", "URL", "å‚™è€ƒ"]
        ws.append(headers)

        # ãƒ‡ãƒ¼ã‚¿
        for store in all_stores:
            ws.append([
                store.company_name,
                store.store_name,
                store.address,
                store.prefecture,
                store.phone,
                store.business_hours,
                store.url,
                store.note,
            ])

        xlsx_output = output_dir / f"åº—èˆ—ä¸€è¦§_{timestamp}.xlsx"
        wb.save(xlsx_output)
        console.print(f"[green]âœ… Excelå‡ºåŠ›: {xlsx_output}[/]")

    except ImportError:
        console.print("[dim]ï¼ˆopenpyxlãŒãªã„ãŸã‚Excelå‡ºåŠ›ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰[/]")

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    console.print("\n[bold]ğŸ“Š æŠ½å‡ºçµæœã‚µãƒãƒªãƒ¼[/]")

    table = Table(show_header=True, header_style="bold magenta")
    table.add_column("ä¼æ¥­å", style="cyan")
    table.add_column("åº—èˆ—æ•°", justify="right")

    # ä¼æ¥­åˆ¥é›†è¨ˆ
    company_counts = {}
    for store in all_stores:
        company_counts[store.company_name] = company_counts.get(store.company_name, 0) + 1

    for company, count in sorted(company_counts.items()):
        table.add_row(company, str(count))

    table.add_row("[bold]åˆè¨ˆ[/]", f"[bold]{len(all_stores)}[/]")

    console.print(table)


if __name__ == "__main__":
    asyncio.run(main())
