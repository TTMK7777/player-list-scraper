#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åº—èˆ—æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ« GUI v2.0
æ”¹å–„ç‰ˆ - å¤šæ®µéšãƒšãƒ¼ã‚¸å·¡å›å¯¾å¿œ
"""

import csv
import json
import os
import re
import io
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import streamlit as st

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åº—èˆ—æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v2",
    page_icon="ğŸª",
    layout="wide"
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
}


# ====================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ====================================
@dataclass
class StoreInfo:
    company_name: str
    store_name: str
    address: str
    phone: str
    url: str
    prefecture: str = ""
    business_hours: str = ""


# ====================================
# åˆæœŸåŒ–
# ====================================
def init_apis():
    """Perplexity APIåˆæœŸåŒ–"""
    from dotenv import load_dotenv
    load_dotenv(Path.home() / ".env.local", override=True)

    api_key = os.getenv("PERPLEXITY_API_KEY")

    if api_key:
        st.sidebar.caption(f"ğŸ”‘ Key: {api_key[:15]}...")

    return api_key


def call_llm(api_key: str, prompt: str) -> str:
    """Perplexity APIå‘¼ã³å‡ºã—"""
    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "sonar",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.1,
                "max_tokens": 8000
            },
            timeout=120
        )
        response.raise_for_status()
        data = response.json()
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        st.error(f"APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return ""


def fetch_page(url: str) -> tuple[str, BeautifulSoup]:
    """ãƒšãƒ¼ã‚¸å–å¾—"""
    response = requests.get(url, headers=HEADERS, timeout=30)
    response.encoding = response.apparent_encoding
    html = response.text
    soup = BeautifulSoup(html, "html.parser")
    return html, soup


# ====================================
# åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸æ¢ç´¢ï¼ˆæ”¹å–„ç‰ˆï¼‰
# ====================================
def find_store_pages(soup: BeautifulSoup, base_url: str, company_name: str, api_key: str) -> list[str]:
    """åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã®å€™è£œã‚’æ¢ç´¢"""

    # ã¾ãšã€ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç›´æ¥ãƒã‚§ãƒƒã‚¯
    common_patterns = [
        "/store", "/stores", "/shop", "/shops", "/studio", "/studios",
        "/location", "/locations", "/branch", "/branches", "/outlet",
        "/tenpo", "/åº—èˆ—", "/access"
    ]

    candidate_urls = set()

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    for a in soup.find_all("a", href=True):
        href = a.get("href", "")
        text = a.get_text(strip=True).lower()

        # åº—èˆ—é–¢é€£ã®ãƒªãƒ³ã‚¯ã‚’æ¤œå‡º
        if any(p in href.lower() for p in common_patterns):
            full_url = urljoin(base_url, href)
            candidate_urls.add(full_url)

        # ãƒ†ã‚­ã‚¹ãƒˆã§æ¤œå‡º
        if any(kw in text for kw in ["åº—èˆ—", "ã‚¢ã‚¯ã‚»ã‚¹", "åº—èˆ—ä¸€è¦§", "åº—èˆ—ç´¹ä»‹", "åº—èˆ—æƒ…å ±"]):
            full_url = urljoin(base_url, href)
            candidate_urls.add(full_url)

    # å€™è£œãŒå¤šã™ãã‚‹å ´åˆã¯LLMã§çµã‚Šè¾¼ã¿
    if len(candidate_urls) > 5:
        links_text = "\n".join([f"- {url}" for url in list(candidate_urls)[:30]])
        prompt = f"""
ä»¥ä¸‹ã¯ã€Œ{company_name}ã€ã®ã‚µã‚¤ãƒˆã«ã‚ã‚‹åº—èˆ—é–¢é€£ã®URLå€™è£œã§ã™ã€‚
åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã¨ã—ã¦æœ€ã‚‚é©åˆ‡ãªURLã‚’æœ€å¤§3ã¤é¸ã‚“ã§ãã ã•ã„ã€‚

ã€URLå€™è£œã€‘
{links_text}

ã€å‡ºåŠ›å½¢å¼ã€‘
URLã®ã¿ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã€‚ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã€‚
"""
        result = call_llm(api_key, prompt)
        if result:
            candidate_urls = set()
            for line in result.strip().split("\n"):
                line = line.strip()
                if line.startswith("http"):
                    candidate_urls.add(line.split()[0])

    return list(candidate_urls)[:5]


# ====================================
# åº—èˆ—æƒ…å ±æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
# ====================================
def extract_stores_from_page(html: str, company_name: str, page_url: str, api_key: str) -> list[StoreInfo]:
    """1ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’æŠ½å‡º"""

    # HTMLãŒå¤§ãã™ãã‚‹å ´åˆã¯é‡è¦éƒ¨åˆ†ã®ã¿æŠ½å‡º
    soup = BeautifulSoup(html, "html.parser")

    # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
    for tag in soup.find_all(["script", "style", "nav", "header", "footer", "noscript"]):
        tag.decompose()

    # åº—èˆ—æƒ…å ±ãŒå«ã¾ã‚Œãã†ãªéƒ¨åˆ†ã‚’æŠ½å‡º
    main_content = soup.find("main") or soup.find("article") or soup.find("div", class_=re.compile(r"content|main|store|shop"))

    if main_content:
        clean_html = str(main_content)
    else:
        clean_html = str(soup.body) if soup.body else html

    # HTMLã‚µã‚¤ã‚ºåˆ¶é™
    if len(clean_html) > 40000:
        clean_html = clean_html[:40000]

    prompt = f"""
ä»¥ä¸‹ã®HTMLã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€ä¼æ¥­åã€‘{company_name}
ã€ãƒšãƒ¼ã‚¸URLã€‘{page_url}

ã€æŠ½å‡ºé …ç›®ã€‘
- store_name: åº—èˆ—åï¼ˆå¿…é ˆï¼‰
- address: ä½æ‰€ï¼ˆéƒµä¾¿ç•ªå·å«ã‚€ï¼‰
- phone: é›»è©±ç•ªå·ï¼ˆãƒã‚¤ãƒ•ãƒ³ä»˜ãï¼‰
- prefecture: éƒ½é“åºœçœŒ
- business_hours: å–¶æ¥­æ™‚é–“
- url: åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸URLï¼ˆç›¸å¯¾ãƒ‘ã‚¹ã®å ´åˆã¯ãã®ã¾ã¾ï¼‰

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
1. ã™ã¹ã¦ã®åº—èˆ—ã‚’æ¼ã‚ŒãªãæŠ½å‡ºã—ã¦ãã ã•ã„
2. åº—èˆ—åã¨ä½æ‰€ã®ä¸¡æ–¹ãŒã‚ã‚‹å ´åˆã®ã¿æŠ½å‡º
3. é‡è¤‡ã¯é™¤å¤–
4. JSONé…åˆ—ã®ã¿å‡ºåŠ›ï¼ˆèª¬æ˜æ–‡ä¸è¦ï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
```json
[
  {{"store_name": "æ¸‹è°·åº—", "address": "ã€’150-0001 æ±äº¬éƒ½æ¸‹è°·åŒº...", "phone": "03-1234-5678", "prefecture": "æ±äº¬éƒ½", "business_hours": "10:00-21:00", "url": "/shop/shibuya"}}
]
```

ã€HTMLã€‘
{clean_html}
"""

    try:
        text = call_llm(api_key, prompt)

        if not text:
            return []

        # JSONæŠ½å‡º
        json_match = re.search(r'\[[\s\S]*\]', text)
        if json_match:
            text = json_match.group()

        data = json.loads(text)

        stores = []
        seen = set()

        for item in data:
            store_name = item.get("store_name", "").strip()
            address = item.get("address", "").strip()

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            key = f"{store_name}_{address}"
            if key in seen or not store_name:
                continue
            seen.add(key)

            # URLæ­£è¦åŒ–
            store_url = item.get("url", "")
            if store_url and not store_url.startswith("http"):
                store_url = urljoin(page_url, store_url)

            store = StoreInfo(
                company_name=company_name,
                store_name=store_name,
                address=address,
                phone=item.get("phone", "").strip(),
                prefecture=item.get("prefecture", "").strip(),
                business_hours=item.get("business_hours", "").strip(),
                url=store_url
            )
            stores.append(store)

        return stores

    except json.JSONDecodeError as e:
        st.warning(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        st.error(f"æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return []


# ====================================
# ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†
# ====================================
def scrape_stores(url: str, company_name: str, api_key: str, status_container) -> list[StoreInfo]:
    """åº—èˆ—æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå¤šæ®µéšå¯¾å¿œï¼‰"""
    all_stores = []
    visited_urls = set()

    try:
        # Step 1: ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸å–å¾—
        status_container.info(f"ğŸŒ ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ä¸­: {url}")
        html, soup = fetch_page(url)
        visited_urls.add(url)

        # Step 2: åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢
        status_container.info("ğŸ” åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢ä¸­...")
        store_pages = find_store_pages(soup, url, company_name, api_key)

        # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸è‡ªä½“ã‚‚å€™è£œã«è¿½åŠ 
        if url not in store_pages:
            store_pages.insert(0, url)

        st.sidebar.write(f"ğŸ“„ æ¢ç´¢ãƒšãƒ¼ã‚¸æ•°: {len(store_pages)}")

        # Step 3: å„ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’æŠ½å‡º
        for i, page_url in enumerate(store_pages):
            if page_url in visited_urls and page_url != url:
                continue
            visited_urls.add(page_url)

            status_container.info(f"ğŸ“ ãƒšãƒ¼ã‚¸ {i+1}/{len(store_pages)}: {page_url[:60]}...")

            try:
                if page_url != url:
                    html, soup = fetch_page(page_url)
                    time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

                status_container.info(f"ğŸ§  åº—èˆ—æƒ…å ±ã‚’æŠ½å‡ºä¸­...")
                stores = extract_stores_from_page(html, company_name, page_url, api_key)

                if stores:
                    st.sidebar.write(f"  â†’ {len(stores)}ä»¶")
                    all_stores.extend(stores)

            except Exception as e:
                st.warning(f"ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {page_url} - {e}")
                continue

        # é‡è¤‡é™¤å»
        seen = set()
        unique_stores = []
        for store in all_stores:
            key = f"{store.store_name}_{store.address}"
            if key not in seen:
                seen.add(key)
                unique_stores.append(store)

        status_container.success(f"âœ… å®Œäº†: {len(unique_stores)}ä»¶ã®åº—èˆ—ã‚’æŠ½å‡º")
        return unique_stores

    except requests.exceptions.Timeout:
        status_container.error("âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: ã‚µã‚¤ãƒˆã®å¿œç­”ãŒé…ã„ã§ã™")
    except requests.exceptions.RequestException as e:
        status_container.error(f"âŒ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
    except Exception as e:
        status_container.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

    return all_stores


# ====================================
# UI
# ====================================
def main():
    st.title("ğŸª åº—èˆ—æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ v2.0")
    st.caption("ä¼æ¥­å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰åº—èˆ—æƒ…å ±ã‚’è‡ªå‹•æŠ½å‡ºï¼ˆå¤šæ®µéšãƒšãƒ¼ã‚¸å·¡å›å¯¾å¿œï¼‰")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")

        api_key = init_apis()
        if api_key:
            st.success("âœ… Perplexity API æ¥ç¶šOK")
        else:
            st.error("âŒ PERPLEXITY_API_KEY ãŒæœªè¨­å®š")
            st.info("~/.env.local ã«è¨­å®šã—ã¦ãã ã•ã„")
            return

        st.divider()
        st.caption("ğŸ’¡ v2.0 æ–°æ©Ÿèƒ½")
        st.caption("- å¤šæ®µéšãƒšãƒ¼ã‚¸å·¡å›")
        st.caption("- åº—èˆ—ä¸€è¦§ãƒšãƒ¼ã‚¸è‡ªå‹•æ¢ç´¢")
        st.caption("- é‡è¤‡è‡ªå‹•é™¤å»")

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“ å…¥åŠ›")

        input_method = st.radio(
            "å…¥åŠ›æ–¹æ³•",
            ["URLç›´æ¥å…¥åŠ›", "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"],
            horizontal=True
        )

        companies = []

        if input_method == "URLç›´æ¥å…¥åŠ›":
            company_name = st.text_input("ä¼æ¥­å", placeholder="ä¾‹: ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¸ã‚ª")
            company_url = st.text_input("å…¬å¼ã‚µã‚¤ãƒˆURL", placeholder="https://www.lifestudio.jp/")

            if company_name and company_url:
                companies = [(company_name, company_url)]

        else:
            uploaded = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«", type=["csv"])
            if uploaded:
                content = uploaded.read().decode("utf-8-sig")
                reader = csv.DictReader(io.StringIO(content))
                for row in reader:
                    name = row.get("ä¼æ¥­å", row.get("name", ""))
                    url = row.get("URL", row.get("url", ""))
                    if name and url:
                        companies.append((name, url))
                st.info(f"{len(companies)}ç¤¾ã‚’èª­ã¿è¾¼ã¿")

        run_button = st.button("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹", type="primary", disabled=not companies)

    with col2:
        st.subheader("ğŸ“Š çµæœ")
        result_area = st.container()

    # å®Ÿè¡Œå‡¦ç†
    if run_button and companies:
        all_stores = []

        with result_area:
            for i, (name, url) in enumerate(companies):
                st.markdown(f"**{i+1}/{len(companies)}: {name}**")
                status_container = st.empty()
                stores = scrape_stores(url, name, api_key, status_container)
                all_stores.extend(stores)

            if all_stores:
                st.divider()
                st.success(f"ğŸ‰ åˆè¨ˆ {len(all_stores)} ä»¶ã®åº—èˆ—ã‚’æŠ½å‡º")

                import pandas as pd
                df = pd.DataFrame([asdict(s) for s in all_stores])
                df.columns = ["ä¼æ¥­å", "åº—èˆ—å", "ä½æ‰€", "é›»è©±ç•ªå·", "URL", "éƒ½é“åºœçœŒ", "å–¶æ¥­æ™‚é–“"]

                st.dataframe(df, use_container_width=True, height=400)

                csv_data = df.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    csv_data,
                    f"åº—èˆ—ä¸€è¦§_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    "text/csv"
                )
            else:
                st.warning("åº—èˆ—æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    main()
