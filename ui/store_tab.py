#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åº—èˆ—èª¿æŸ» UIã‚¿ãƒ–
================
app_v5.py ã‹ã‚‰åº—èˆ—èª¿æŸ»é–¢é€£UIã‚’åˆ†é›¢ã€‚
attribute_tab.py ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æº–æ‹ ã€‚
"""

import io
import tempfile
from datetime import datetime
from pathlib import Path

import pandas as pd
import streamlit as st

from core.async_helpers import run_async
from core.excel_handler import ExcelHandler, StoreInvestigationExporter
from core.llm_client import LLMClient
from investigators.base import StoreInvestigationResult
from investigators.store_investigator import StoreInvestigator, InvestigationMode
from ui.common import display_progress_log


# ====================================
# å†…éƒ¨é–¢æ•°
# ====================================
async def _run_investigation(
    companies: list[dict],
    mode: InvestigationMode,
    progress_container,
    status_container,
    ai_model: str = "gemini-2.5-flash",
) -> list[StoreInvestigationResult]:
    """åº—èˆ—èª¿æŸ»ã‚’å®Ÿè¡Œ"""

    logs: list[str] = []

    def on_progress(current: int, total: int, name: str) -> None:
        log_msg = f"[{current}/{total}] èª¿æŸ»ä¸­: {name}"
        logs.append(log_msg)
        display_progress_log(logs, progress_container)

    model_label = "ç²¾å¯†" if ai_model == "gemini-2.5-pro" else "é«˜é€Ÿ"
    status_container.info(f"ğŸª {len(companies)}ä»¶ã®ä¼æ¥­ã‚’èª¿æŸ»ä¸­... (ãƒ¢ãƒ¼ãƒ‰: {model_label})")

    try:
        # åº—èˆ—èª¿æŸ»ã¯çŸ­æ™‚é–“ã§å¤‰ã‚ã‚‰ãªã„ã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹
        llm = LLMClient(enable_cache=True)
        investigator = StoreInvestigator(llm_client=llm, model=ai_model)

        results = await investigator.investigate_batch(
            companies,
            mode=mode,
            on_progress=on_progress,
            delay_seconds=1.5,
        )

        status_container.success(f"âœ… èª¿æŸ»å®Œäº†: {len(results)}ä»¶")
        return results

    except Exception as e:
        status_container.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {str(e)}")
        return []


def _display_summary(results: list[StoreInvestigationResult]):
    """åº—èˆ—èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""

    total_stores = sum((r.total_stores or 0) for r in results)
    need_verify = sum(1 for r in results if r.needs_verification)

    st.markdown("### ğŸ“Š åº—èˆ—èª¿æŸ»çµæœã‚µãƒãƒªãƒ¼")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("èª¿æŸ»ä¼æ¥­æ•°", f"{len(results)}ä»¶")

    with col2:
        st.metric("ç·åº—èˆ—æ•°", f"{total_stores:,}åº—èˆ—")

    with col3:
        st.metric("âš ï¸ è¦ç¢ºèª", f"{need_verify}ä»¶")


def _display_table(results: list[StoreInvestigationResult]) -> None:
    """åº—èˆ—èª¿æŸ»çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä»˜ãã§è¡¨ç¤ºã€‚

    Args:
        results: åº—èˆ—èª¿æŸ»çµæœã®ãƒªã‚¹ãƒˆã€‚
    """
    # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ---
    show_verify_only = st.checkbox(
        "è¦ç¢ºèªã®ã¿è¡¨ç¤º",
        value=False,
        key="store_filter_verify",
    )

    # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ ---
    filtered_results = [
        r for r in results
        if not show_verify_only or r.needs_verification
    ]

    st.caption(f"è¡¨ç¤ºä¸­: {len(filtered_results)} / {len(results)} ä»¶")

    # --- ã‚½ãƒ¼ãƒˆ & ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º ---
    sorted_results = sorted(
        filtered_results,
        key=lambda r: (r.needs_verification, r.company_name),
    )

    data = []
    for result in sorted_results:
        data.append({
            "ä¼æ¥­å": result.company_name,
            "åº—èˆ—æ•°": result.total_stores or 0,
            "ç›´å–¶åº—": result.direct_stores if result.direct_stores is not None else "-",
            "FCåº—": result.franchise_stores if result.franchise_stores is not None else "-",
            "èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰": result.investigation_mode,
            "è¦ç¢ºèª": "âš ï¸" if result.needs_verification else "",
            "ã‚½ãƒ¼ã‚¹URL": ", ".join(result.source_urls[:2]) if result.source_urls else "-",
        })

    df = pd.DataFrame(data)

    st.dataframe(
        df,
        use_container_width=True,
        height=400,
        column_config={
            "ä¼æ¥­å": st.column_config.TextColumn("ä¼æ¥­å", width="medium"),
            "åº—èˆ—æ•°": st.column_config.NumberColumn("åº—èˆ—æ•°", width="small"),
            "ç›´å–¶åº—": st.column_config.TextColumn("ç›´å–¶åº—", width="small"),
            "FCåº—": st.column_config.TextColumn("FCåº—", width="small"),
            "èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰": st.column_config.TextColumn("èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰", width="small"),
            "è¦ç¢ºèª": st.column_config.TextColumn("è¦ç¢ºèª", width="small"),
            "ã‚½ãƒ¼ã‚¹URL": st.column_config.TextColumn("ã‚½ãƒ¼ã‚¹URL", width="large"),
        },
    )


def _export_results(results: list[StoreInvestigationResult]) -> bytes:
    """åº—èˆ—èª¿æŸ»çµæœã‚’Excelã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""

    buffer = io.BytesIO()

    temp_path = Path(tempfile.gettempdir()) / "store_results_temp.xlsx"
    exporter = StoreInvestigationExporter(include_prefectures=True)
    exporter.export(results, temp_path)

    with open(temp_path, "rb") as f:
        buffer.write(f.read())

    temp_path.unlink(missing_ok=True)

    return buffer.getvalue()


def _display_scraping_warning():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®æ³¨æ„äº‹é …ã‚’è¡¨ç¤º"""
    st.markdown("""
    <div class="warning-box">
    <h4>âš ï¸ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®æ³¨æ„äº‹é …</h4>
    <ul>
        <li>å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšã”ç¢ºèªãã ã•ã„</li>
        <li>robots.txt ã§ç¦æ­¢ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„</li>
        <li>æœ¬æ©Ÿèƒ½ã®ä½¿ç”¨ã«ã‚ˆã‚‹æ³•çš„å•é¡Œã¯åˆ©ç”¨è€…ã®è²¬ä»»ã¨ãªã‚Šã¾ã™</li>
        <li>ç¤¾å†…åˆ©ç”¨ã®ã¿ã‚’æ¨å¥¨ã—ã¾ã™ï¼ˆå¤–éƒ¨å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã¸ã®ä½¿ç”¨ã¯éæ¨å¥¨ï¼‰</li>
    </ul>
    </div>
    """, unsafe_allow_html=True)


def _display_company_detail(result: StoreInvestigationResult):
    """ä¼æ¥­åˆ¥è©³ç´°ã‚’è¡¨ç¤º"""
    stores_display = result.total_stores or 0
    with st.expander(f"{'âš ï¸' if result.needs_verification else 'âœ…'} {result.company_name} - {stores_display}åº—èˆ—"):
        col1, col2 = st.columns(2)

        with col1:
            st.write("**åŸºæœ¬æƒ…å ±**")
            st.write(f"- ç·åº—èˆ—æ•°: {stores_display}")
            if result.direct_stores is not None:
                st.write(f"- ç›´å–¶åº—: {result.direct_stores}")
            if result.franchise_stores is not None:
                st.write(f"- FCåº—: {result.franchise_stores}")
            st.write(f"- èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰: {result.investigation_mode}")

        with col2:
            st.write("**æƒ…å ±ã‚½ãƒ¼ã‚¹**")
            if result.source_urls:
                for url in result.source_urls:
                    st.write(f"- {url}")
            else:
                st.write("- ãªã—")

        if result.prefecture_distribution:
            st.write("**éƒ½é“åºœçœŒåˆ¥åº—èˆ—æ•°**")
            pref_df = pd.DataFrame([
                {"éƒ½é“åºœçœŒ": k, "åº—èˆ—æ•°": v}
                for k, v in result.prefecture_distribution.items()
            ]).sort_values("åº—èˆ—æ•°", ascending=False)
            st.dataframe(pref_df, use_container_width=True, height=200)

        if result.notes:
            st.write("**å‚™è€ƒ**")
            st.write(result.notes)


# ====================================
# ãƒ¡ã‚¤ãƒ³ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
# ====================================
def render_store_tab():
    """åº—èˆ—èª¿æŸ»ã‚¿ãƒ–ã®UIã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°"""

    st.info("ä¼æ¥­ã®**åº—èˆ—ãƒ»æ•™å®¤æ•°**ã‚’éƒ½é“åºœçœŒåˆ¥ã«èª¿æŸ»ã—ã¾ã™ã€‚AIèª¿æŸ»ï¼ˆæ¨å¥¨ï¼‰ã¾ãŸã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–å¾—ã—ã¾ã™ã€‚")

    # ã‚¿ãƒ–å›ºæœ‰ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹åˆæœŸåŒ–
    if "store_companies" not in st.session_state:
        st.session_state.store_companies = []
    if "store_results" not in st.session_state:
        st.session_state.store_results = []
    if "store_is_running" not in st.session_state:
        st.session_state.store_is_running = False

    st.subheader("ğŸ”§ èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰é¸æŠ")

    mode_option = st.radio(
        "èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰",
        [
            "ğŸ¤– AIèª¿æŸ»ï¼ˆé«˜é€Ÿï¼‰",
            "ğŸ”¬ AIèª¿æŸ»ï¼ˆç²¾å¯†ï¼‰",
            "ğŸ”— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°",
            "ğŸ”„ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆAI + ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è£œå®Œï¼‰",
        ],
        horizontal=True,
        label_visibility="collapsed",
    )

    # ãƒ¢ãƒ¼ãƒ‰å¤‰æ› & ãƒ¢ãƒ‡ãƒ«é¸æŠ
    ai_model = "gemini-2.5-flash"

    if "AIèª¿æŸ»ï¼ˆé«˜é€Ÿï¼‰" in mode_option:
        investigation_mode = InvestigationMode.AI
        ai_model = "gemini-2.5-flash"
    elif "AIèª¿æŸ»ï¼ˆç²¾å¯†ï¼‰" in mode_option:
        investigation_mode = InvestigationMode.AI
        ai_model = "gemini-2.5-pro"
        st.warning(
            "â³ **ç²¾å¯†ãƒ¢ãƒ¼ãƒ‰ï¼ˆgemini-2.5-proï¼‰ã®æ³¨æ„äº‹é …**\n\n"
            "- é«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ã‚ˆã‚Šå¿œç­”ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™\n"
            "- ã‚³ã‚¹ãƒˆãŒé«˜ããªã‚Šã¾ã™\n"
            "- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§ `?` ãŒå¤šã„å ´åˆã®ã¿æ¨å¥¨\n\n"
            "ã¾ãšã¯ã€ŒAIèª¿æŸ»ï¼ˆé«˜é€Ÿï¼‰ã€ã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚"
        )
    elif "ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°" in mode_option:
        investigation_mode = InvestigationMode.SCRAPING
    else:
        investigation_mode = InvestigationMode.HYBRID

    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ³¨æ„äº‹é …
    if investigation_mode in (InvestigationMode.SCRAPING, InvestigationMode.HYBRID):
        _display_scraping_warning()

    st.divider()

    # å…¥åŠ›ã‚¿ãƒ–
    st.subheader("ğŸ“‚ ä¼æ¥­æƒ…å ±å…¥åŠ›")

    input_tab1, input_tab2 = st.tabs(["ğŸ“¤ Excelã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "âœï¸ ç›´æ¥å…¥åŠ›"])

    with input_tab1:
        uploaded_file = st.file_uploader(
            "ä¼æ¥­ãƒªã‚¹ãƒˆExcelã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=["xlsx", "xls"],
            help="ä¼æ¥­åã€å…¬å¼URL ã‚’å«ã‚€Excelãƒ•ã‚¡ã‚¤ãƒ«",
            key="store_excel_upload",
        )

        if uploaded_file:
            try:
                temp_dir = Path(tempfile.gettempdir()) / "store_investigator"
                temp_dir.mkdir(parents=True, exist_ok=True)
                temp_path = temp_dir / uploaded_file.name
                temp_path.write_bytes(uploaded_file.getvalue())

                handler = ExcelHandler()
                players = handler.load(temp_path)

                companies = []
                for p in players:
                    companies.append({
                        "company_name": p.company_name or p.player_name,
                        "official_url": p.official_url,
                        "industry": "",
                    })

                st.session_state.store_companies = companies
                st.success(f"âœ… {len(companies)}ä»¶ã®ä¼æ¥­ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                with st.expander("ğŸ‘€ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå…ˆé ­10ä»¶ï¼‰"):
                    preview_data = []
                    for c in companies[:10]:
                        preview_data.append({
                            "ä¼æ¥­å": c["company_name"],
                            "å…¬å¼URL": c["official_url"][:50] + "..." if len(c["official_url"]) > 50 else c["official_url"],
                        })
                    st.dataframe(pd.DataFrame(preview_data), use_container_width=True)

            except Exception as e:
                st.error(f"âŒ Excelã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                st.session_state.store_companies = []

    with input_tab2:
        st.markdown("**ä¼æ¥­æƒ…å ±ã‚’ç›´æ¥å…¥åŠ›**ï¼ˆ1è¡Œ1ä¼æ¥­ï¼‰")

        input_text = st.text_area(
            "ä¼æ¥­å\tå…¬å¼URL ã®å½¢å¼ã§å…¥åŠ›ï¼ˆã‚¿ãƒ–åŒºåˆ‡ã‚Š or ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
            placeholder="ã‚¹ã‚¿ãƒ¼ãƒãƒƒã‚¯ã‚¹\thttps://www.starbucks.co.jp/\nãƒ‰ãƒˆãƒ¼ãƒ«\thttps://www.doutor.co.jp/",
            height=150,
        )

        if st.button("ğŸ“ å…¥åŠ›å†…å®¹ã‚’åæ˜ ", key="apply_direct_input"):
            companies = []
            for line in input_text.strip().split("\n"):
                if not line.strip():
                    continue
                # ã‚¿ãƒ–åŒºåˆ‡ã‚Šã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ã‚«ãƒ³ãƒã®æœ€å¾Œã®å‡ºç¾ä½ç½®ã§åˆ†å‰²
                if "\t" in line:
                    parts = line.split("\t", 1)
                elif ",http" in line:
                    # URLéƒ¨åˆ†ã‚’å®‰å…¨ã«åˆ†é›¢ï¼ˆä¼æ¥­åã«ã‚«ãƒ³ãƒãŒå«ã¾ã‚Œã‚‹ã‚±ãƒ¼ã‚¹å¯¾å¿œï¼‰
                    idx = line.index(",http")
                    parts = [line[:idx], line[idx + 1:]]
                else:
                    parts = line.split(",", 1)
                company_name = parts[0].strip()
                official_url = parts[1].strip() if len(parts) > 1 else ""

                companies.append({
                    "company_name": company_name,
                    "official_url": official_url,
                    "industry": industry,
                })

            if companies:
                st.session_state.store_companies = companies
                st.success(f"âœ… {len(companies)}ä»¶ã®ä¼æ¥­ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
            else:
                st.warning("âš ï¸ ä¼æ¥­æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    st.divider()

    # èª¿æŸ»å®Ÿè¡Œ
    col1, col2 = st.columns([1, 3])
    with col1:
        check_limit = st.number_input(
            "èª¿æŸ»ä»¶æ•°",
            min_value=1,
            max_value=len(st.session_state.store_companies) if st.session_state.store_companies else 100,
            value=min(5, len(st.session_state.store_companies)) if st.session_state.store_companies else 5,
            help="APIã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã€æœ€åˆã¯å°‘æ•°ã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„",
            key="store_check_limit",
        )

    with col2:
        run_button = st.button(
            "ğŸš€ åº—èˆ—èª¿æŸ»é–‹å§‹",
            type="primary",
            disabled=not st.session_state.store_companies or st.session_state.store_is_running,
            use_container_width=True,
            key="store_run_button",
        )

    st.divider()

    if run_button:
        st.session_state.store_is_running = True

        progress_container = st.empty()
        status_container = st.empty()

        companies_to_check = st.session_state.store_companies[:check_limit]

        try:
            results = run_async(_run_investigation(
                companies_to_check,
                mode=investigation_mode,
                progress_container=progress_container,
                status_container=status_container,
                ai_model=ai_model,
            ))

            st.session_state.store_results = results
        except Exception as e:
            st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {type(e).__name__}: {e}")
        finally:
            st.session_state.store_is_running = False

    # çµæœè¡¨ç¤º
    if st.session_state.store_results:
        results = st.session_state.store_results

        _display_summary(results)

        st.divider()

        st.subheader("ğŸ“‹ è©³ç´°çµæœ")
        _display_table(results)

        st.divider()

        st.subheader("ğŸ“¥ çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")

        col1, col2 = st.columns(2)

        with col1:
            excel_data = _export_results(results)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            st.download_button(
                "ğŸ“¥ Excel ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåº—èˆ—èª¿æŸ»çµæœï¼‰",
                excel_data,
                f"store_investigation_{timestamp}.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )

        with col2:
            csv_data = []
            for result in results:
                csv_data.append(result.to_dict())
            df_csv = pd.DataFrame(csv_data)
            csv_bytes = df_csv.to_csv(index=False).encode("utf-8-sig")

            st.download_button(
                "ğŸ“¥ CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                csv_bytes,
                f"store_investigation_{timestamp}.csv",
                "text/csv",
                use_container_width=True,
            )

        # è©³ç´°è¡¨ç¤º
        st.divider()
        st.subheader("ğŸ“ ä¼æ¥­åˆ¥è©³ç´°")

        for result in results:
            _display_company_detail(result)
